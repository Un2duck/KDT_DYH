{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분류 모델\n",
    "from torch import nn\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_vocab,\n",
    "                 hidden_dim,\n",
    "                 embedding_dim,\n",
    "                 n_layers,\n",
    "                 dropout=0.5,\n",
    "                 bidirectional=True,\n",
    "                 model_type='lstm'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터세트 불러오기\n",
    "\n",
    "import pandas as pd\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-50\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-50\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('nsmc')\n",
    "corpus_df = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = corpus_df.sample(frac=0.9, random_state=42)\n",
    "test = corpus_df.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "print(train.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train))\n",
    "print(\"Testing Data Size :\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 토큰화 및 단어 사전 구축\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩 및 패딩\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = token_to_id[\"<unk>\"]\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "    ]\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "pad_id = token_to_id['<pad>']\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 적용\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_lables = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_lables = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_lables)\n",
    "test_dataset = TensorDataset(test_ids, test_lables)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수와 최적화 함수 정의\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 테스트\n",
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f'Train Loss {step}: {np.mean(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "    print(f'Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0: 0.7001947164535522\n",
      "Train Loss 500: 0.693438629904193\n",
      "Train Loss 1000: 0.692995163527402\n",
      "Train Loss 1500: 0.6807639113987866\n",
      "Train Loss 2000: 0.6570099064822438\n",
      "Train Loss 2500: 0.6348229087099748\n",
      "Val Loss : 0.5196410457071025, Val Accuracy : 0.761\n",
      "Train Loss 0: 0.622211217880249\n",
      "Train Loss 500: 0.4764801027056224\n",
      "Train Loss 1000: 0.47221748003890585\n",
      "Train Loss 1500: 0.46395730927119966\n",
      "Train Loss 2000: 0.45882484196559126\n",
      "Train Loss 2500: 0.45338339929948185\n",
      "Val Loss : 0.42812847015195, Val Accuracy : 0.8006\n",
      "Train Loss 0: 0.4823038578033447\n",
      "Train Loss 500: 0.37774101536193055\n",
      "Train Loss 1000: 0.37726656680221443\n",
      "Train Loss 1500: 0.37932356872870715\n",
      "Train Loss 2000: 0.37941422996700913\n",
      "Train Loss 2500: 0.37917396082121674\n",
      "Val Loss : 0.421879488105972, Val Accuracy : 0.796\n",
      "Train Loss 0: 0.3029881417751312\n",
      "Train Loss 500: 0.33133462168082983\n",
      "Train Loss 1000: 0.33316914476714765\n",
      "Train Loss 1500: 0.33145839197785754\n",
      "Train Loss 2000: 0.33323383683535174\n",
      "Train Loss 2500: 0.33664159944168526\n",
      "Val Loss : 0.3940518250861488, Val Accuracy : 0.812\n",
      "Train Loss 0: 0.38939380645751953\n",
      "Train Loss 500: 0.29380362944867083\n",
      "Train Loss 1000: 0.2975395589091859\n",
      "Train Loss 1500: 0.3017328568832883\n",
      "Train Loss 2000: 0.29985320715997915\n",
      "Train Loss 2500: 0.2999033075561825\n",
      "Val Loss : 0.4078489311586935, Val Accuracy : 0.8246\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "interval = 500\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [-2.7252326  -1.6364336   0.2682625   0.9603476   1.2904423  -2.8310812\n",
      " -0.44858474  0.53599757  0.51257706  0.12093402  1.0669913   1.146301\n",
      " -0.6921513   0.5708175   1.6692575  -1.5926312  -1.4043725   2.3508148\n",
      " -2.5604475   0.2849467  -2.2112677   0.92306584  0.14503534 -0.5120045\n",
      "  1.6272874   0.02130163  0.10072117 -0.78453374  0.08679244 -1.0180925\n",
      "  2.1305308  -0.53132004 -0.61496264  0.6868998  -0.91903543 -1.2498301\n",
      " -0.50619674  1.2894588  -1.0364957  -0.09542676  1.1538835   0.20951161\n",
      " -0.6741956  -0.6324141  -0.1256448  -0.7929636  -0.06957742 -0.72102255\n",
      " -0.2644245  -1.1001936   0.78355145  0.21204749  0.90505666 -1.6403633\n",
      "  0.5900494  -0.47923645  0.84786457  0.27278078  0.7322763   1.2687445\n",
      "  0.531476    0.8172104  -0.5445721   0.26276433  2.507312   -0.4791292\n",
      " -1.3700311   1.0872015   0.51680446  1.3758634   0.6327788  -0.5940596\n",
      "  0.8857784  -0.88710934 -1.6362709  -0.17476794  0.04585332  0.14498466\n",
      " -0.7323526   0.02354389 -1.3686942  -0.8400328  -0.03826766 -0.28134358\n",
      " -0.45953116 -1.1938941   0.4785086  -1.61163    -0.2003852   0.44412684\n",
      " -0.83859754  0.82732755 -0.67372596  1.0835547   0.32833177 -1.8400981\n",
      "  2.2347775  -0.5604665   0.2913722   0.4073072   0.5656527   2.7563143\n",
      " -1.3581263   0.9711405  -0.59442437 -0.58877844  0.29933333  2.0609968\n",
      " -0.95124996  0.09502956  0.4279672  -0.48965004 -0.83350194 -0.502939\n",
      "  0.42201108  0.47126928 -1.2297758  -0.3193006  -0.5305463  -2.5972126\n",
      " -0.8440245   0.05359114  0.9547151  -1.1554861   1.2103431  -1.1646298\n",
      " -0.3886846  -0.6364437 ]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델로부터 임베딩 추출\n",
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습된 모델로 임베딩 계층 초기화\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load('../models/word2vec.model')\n",
    "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
    "\n",
    "for index, token in id_to_token.items():\n",
    "    if token not in ['<pad>','<unk>']:\n",
    "        init_embeddings[index] = word2vec.wv[token]\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습된 임베딩 계층 적용\n",
    "\n",
    "class SentenceClassifier2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_vocab,\n",
    "                 hidden_dim,\n",
    "                 embedding_dim,\n",
    "                 n_layers,\n",
    "                 dropout=0.5,\n",
    "                 bidirectional=True,\n",
    "                 model_type='lstm',\n",
    "                 pretrained_embedding=None\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=n_vocab,\n",
    "                embedding_dim=embedding_dim,\n",
    "                padding_idx=0\n",
    "            )\n",
    "\n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습된 임베딩을 사용한 모델 학습\n",
    "classifier2 = SentenceClassifier2(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim,\n",
    "    n_layers=n_layers, pretrained_embedding=init_embeddings\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0: 0.6844468116760254\n",
      "Train Loss 500: 0.6665256898441239\n",
      "Train Loss 1000: 0.6643826766804858\n",
      "Train Loss 1500: 0.6460254401028117\n",
      "Train Loss 2000: 0.6330621158373707\n",
      "Train Loss 2500: 0.6146822075088803\n",
      "Val Loss : 0.49753828422901347, Val Accuracy : 0.7704\n",
      "Train Loss 0: 0.6750407218933105\n",
      "Train Loss 500: 0.526442036687019\n",
      "Train Loss 1000: 0.5409470669307432\n",
      "Train Loss 1500: 0.5406075732796133\n",
      "Train Loss 2000: 0.5430256086683226\n",
      "Train Loss 2500: 0.5399326432149251\n",
      "Val Loss : 0.5102891431639369, Val Accuracy : 0.7566\n",
      "Train Loss 0: 0.4817048907279968\n",
      "Train Loss 500: 0.5146990398506442\n",
      "Train Loss 1000: 0.5090822963775335\n",
      "Train Loss 1500: 0.5040682891382526\n",
      "Train Loss 2000: 0.49618661636742634\n",
      "Train Loss 2500: 0.49410975490008197\n",
      "Val Loss : 0.47849095072418735, Val Accuracy : 0.7824\n",
      "Train Loss 0: 0.6110131740570068\n",
      "Train Loss 500: 0.4788760099106444\n",
      "Train Loss 1000: 0.47007176603053835\n",
      "Train Loss 1500: 0.467499750701687\n",
      "Train Loss 2000: 0.46518161624059384\n",
      "Train Loss 2500: 0.46205711449350845\n",
      "Val Loss : 0.4384695645243215, Val Accuracy : 0.7988\n",
      "Train Loss 0: 0.2571444511413574\n",
      "Train Loss 500: 0.4376977578013719\n",
      "Train Loss 1000: 0.4376694564367984\n",
      "Train Loss 1500: 0.435408900020958\n",
      "Train Loss 2000: 0.43691230924992724\n",
      "Train Loss 2500: 0.4352298182417087\n",
      "Val Loss : 0.4247751288330212, Val Accuracy : 0.7946\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "interval = 500\n",
    "for epoch in range(epochs):\n",
    "    train(classifier2, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier2, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_TORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
