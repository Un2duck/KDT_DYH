{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 코퍼스 전처리 (*Torchtext 없이 해보기)\n",
    "- 데이터셋 : Korpora에서 로딩\n",
    "- 형태소분석기 설정\n",
    "- 단어사전 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비 <HR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로딩\n",
    "from Korpora import Korpora                         # Open Korean Dataset\n",
    "from konlpy.tag import *                            # 형태소 분석기\n",
    "import spacy                                        # 형태소 분석기\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader    # Pytorch Dataset 관련 모듈\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-50\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-50\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "### 데이터 로딩\n",
    "nsmc = Korpora.load('nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsmc.train[0], nsmc.test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    50000 non-null  object\n",
      " 1   label   50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Test 데이터셋을 DataFrame으로 로딩\n",
    "nsmcDF = pd.DataFrame(nsmc.test)\n",
    "nsmcDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 텍스트 데이터 전처리 <hr>\n",
    "- 토큰화 / 정제 (불용어, 구두점, 띄어쓰기, 오타 등등 처리)\n",
    "- 단어사전 생성\n",
    "- 문장 ==> 수치화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 형태소 분석기\n",
    "# # 한나눔\n",
    "# han = Hannanum()\n",
    "\n",
    "# # 꼬꼬마\n",
    "# kkma = Kkma()\n",
    "\n",
    "# # OKT\n",
    "# okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = nsmc.test.texts[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sample[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 불용어 & 구두점 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text=[]\n",
    "for text in sample[::-1]:\n",
    "    text=text.replace('\\n','')\n",
    "    text=text.replace(punc,'')\n",
    "    if len(text): all_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenList in all_text[:10]:\n",
    "    print(tokenList, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makestr=\",\".join(nsmc.test.texts)\n",
    "makestr=\",\".join(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어,구두점 제거 (정규식) \n",
    "import re\n",
    "import string\n",
    "\n",
    "# newmake = re.sub('[\\,.? ]', '', makestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석\n",
    "\n",
    "# han_result=han.morphs(makestr)\n",
    "# kkma_result=kkma.morphs(makestr)\n",
    "okt_result=okt.morphs(makestr)\n",
    "# okt_result=okt.morphs(newmake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens=[]\n",
    "for text in okt_result:\n",
    "    all_tokens.append( wordpunct_tokenize(text) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 불용어 제거\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_PATH = '../Data/stopwords-ko.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(STOP_PATH, 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytoken = remove_stopwords(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenList in mytoken:\n",
    "    for token in tokenList:\n",
    "        if (token in punc) & (token == 'ㅋ'):\n",
    "            tokenList.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenList in mytoken:\n",
    "    print(tokenList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어별 빈도수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰을 키로 해서 빈도수 저장\n",
    "token_freg={} # dict에서는 key값을 찾아봄\n",
    "\n",
    "# 라인(줄)별 토큰을 읽어서 빈도 체크\n",
    "for tokenList in mytoken:\n",
    "    for token in tokenList:\n",
    "        # 카운트시, 토큰 key가 존재하지 않으면 key 추가\n",
    "        if token not in token_freg:\n",
    "            token_freg[token] = 1\n",
    "        # 카운트시, 이미 존재하는 토큰 key는 1 증가\n",
    "        else:\n",
    "            token_freg[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어들 별로 빈도수 저장\n",
    "freqsDict={}\n",
    "for k, v in token_freg.items():\n",
    "    if v not in freqsDict:\n",
    "        freqsDict[v]=[k]\n",
    "    else:\n",
    "        freqsDict[v].append(k)\n",
    "print(freqsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어등장 횟수, 단어 개수 및 단어이름 별로 빈도수 저장\n",
    "freqsDict={}\n",
    "for k, v in token_freg.items():\n",
    "    if v not in freqsDict:\n",
    "        # freqsDict[v]=[k]\n",
    "        # freqsDict[v]=1\n",
    "        freqsDict[v]=[1, [k]]\n",
    "    else:\n",
    "        # freqsDict[v].append(k)\n",
    "        # freqsDict[v]+=1\n",
    "        freqsDict[v][0]+=1\n",
    "        freqsDict[v][1].append(k)\n",
    "print(freqsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(단어나온 횟수, [단어개수, [단어이름]]) ...]\n",
    "sorted(freqsDict.items(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 빈도가 높게 나오는 순서대로 단어 정렬\n",
    "#                k, v -----------------------------|\n",
    "storedTokens=sorted(token_freg.items(), reverse=True, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어사전 생성 및 초기화\n",
    "## 특수토큰 : 'PAD', 'OOV' 또는 'UNK'\n",
    "PAD_TOKEN, OOV_TOKEN='PAD', 'OOV'\n",
    "vocab={PAD_TOKEN:0, OOV_TOKEN:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 코퍼스에서 추출된 토큰(단어)들\n",
    "# vocab[]\n",
    "\n",
    "for idx, tk in enumerate(storedTokens, 2):\n",
    "    vocab[tk[0]] = idx\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 텍스트 문장 ===> 수치화 [인코딩]\n",
    "encodingData=[]\n",
    "for tokenList in all_tokens:\n",
    "    sent=[]\n",
    "    print(f'문장: {tokenList}')\n",
    "    for token in tokenList:\n",
    "        sent.append(vocab[token])\n",
    "    \n",
    "    # 인코딩 된 문장 저장\n",
    "    encodingData.append(sent)\n",
    "    print(f'==>인코딩: {sent}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_TORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
