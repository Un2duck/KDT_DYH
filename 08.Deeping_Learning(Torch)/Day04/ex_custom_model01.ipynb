{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 모델 클래스\n",
    "- 부모클래스 : nn.module\n",
    "- 필수오버라이딩\n",
    "    * __ init __(): 모델 층 구성 즉, 설계\n",
    "    * forward(): 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로딩\n",
    "import torch                                # 텐서 관련 모듈\n",
    "import torch.nn as nn                       # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F             # 인공신경망 관련 함수들 모듈 ( 손실함수, 활성화 함수 등등 )\n",
    "import torch.optim as optim                 # 최적화 관련 모듈 ( 가중치, 절편 빠르게 찾아주는 알고리즘 )\n",
    "from torchinfo import summary               # 모델 구조 및 정보 관련 모듈\n",
    "from torchmetrics.regression import *       # 회귀 성능 지표 관련 모듈\n",
    "from torchmetrics.classification import *   # 분류 성능 지표 관련 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망클래스 <hr>\n",
    "    * 입력층 - 입력 피쳐 고정\n",
    "    * 출력층 - 출력 타겟 고정\n",
    "    * 은닉층 - 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델설계\n",
    "# 데이터셋 : 피쳐 4개,   타겟 1개,   회귀\n",
    "# 입 력 층 : 입력 4개    출력 20개   AF ReLU\n",
    "# 은 닉 층 : 입력 20개   출력 100개  AF ReLU\n",
    "# 출 력 층 : 입력 100개  출력 1개    AF X, (분류 -> Sigmoid(2진) & Softmax(다중))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 callback func)\n",
    "    def __init__(self):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer=nn.Linear(4, 20)       # w: 4 + b: 1 => 1P(Perceptron), 5 * 20 = 100개 변수\n",
    "        self.hidden_layer=nn.Linear(20, 100)    # w: 20 + b: 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer=nn.Linear(100, 1)     # w: 100 + b: 1 => 1P, 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, dataset):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(dataset) # 1개 퍼셉트론 : y=x1w1+x2w2+x3w3+x4w4+b\n",
    "        y=F.relu(y)                   # 0<=y ----> 죽은 ReLU ==> LeakyReLU\n",
    "\n",
    "        y=self.hidden_layer(y) # y=x1w1+y2w2+~~~+x20w20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # y=x1w1+y2w2+~~~+x100w100+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델\n",
    "class MyModel2(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 callback func)\n",
    "    def __init__(self, in_feature):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer=nn.Linear(in_feature, 20)       # w: 4 + b: 1 => 1P(Perceptron), 5 * 20 = 100개 변수\n",
    "        self.hidden_layer=nn.Linear(20, 100)    # w: 20 + b: 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer=nn.Linear(100, 1)     # w: 100 + b: 1 => 1P, 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, dataset):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(dataset) # 1개 퍼셉트론 : y=x1w1+x2w2+x3w3+x4w4+b\n",
    "        y=F.relu(y)                 # 0<=y ----> 죽은 ReLU ==> LeakyReLU\n",
    "\n",
    "        y=self.hidden_layer(y) # y=x1w1+y2w2+~~~+x20w20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # y=x1w1+y2w2+~~~+x100w100+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수, 은닉층 퍼셉트론 수가 동적인 모델\n",
    "class MyModel3(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 callback func)\n",
    "    def __init__(self, in_feature, in_out, h_out):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer=nn.Linear(in_feature, in_out)  # w: in_feature 수 + b: 1 => 1P(Perceptron), 5 * 20 = 100개 변수\n",
    "        self.hidden_layer=nn.Linear(in_out, h_out)      # w: in_out 수 + b: 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer=nn.Linear(h_out, 1)           # w: h_out 수 + b: 1 => 1P, 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, dataset):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(dataset) # 1개 퍼셉트론 : y=x1w1+x2w2+x3w3+x4w4+b\n",
    "        y=F.relu(y)                   # 0<=y ----> 죽은 ReLU ==> LeakyReLU\n",
    "        \n",
    "        y=self.hidden_layer(y) # y=x1w1+y2w2+~~~+x20w20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # y=x1w1+y2w2+~~~+x100w100+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class MyModel4(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 callback func)\n",
    "    def __init__(self, in_feature, in_out, h_out, hidden_layers_num=1):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer=nn.Linear(in_feature, in_out)\n",
    "        self.hidden_layers=nn.ModuleDict([nn.Linear(in_out, h_out) for _ in range(hidden_layers_num)])\n",
    "        # self.hidden_layer=nn.Linear(in_out, h_out)\n",
    "        self.output_layer=nn.Linear(h_out, 1)\n",
    "        \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, dataset):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(dataset) # 1개 퍼셉트론 : y=x1w1+x2w2+x3w3+x4w4+b\n",
    "        y=F.relu(y)                   # 0<=y ----> 죽은 ReLU ==> LeakyReLU\n",
    "        \n",
    "        y=self.hidden_layer(y) # y=x1w1+y2w2+~~~+x20w20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # y=x1w1+y2w2+~~~+x100w100+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 인스턴스 생성\n",
    "# ml=MyModel()\n",
    "# ml=MyModel2(4)\n",
    "ml=MyModel3(4, 50, 30)\n",
    "\n",
    "# ml=MyModel4(4, 50, 30)\n",
    "# ml=MyModel4(4, 50, 30, hidden_layers_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2576, -0.2207, -0.0969,  0.2347],\n",
      "        [-0.4707,  0.2999, -0.1029,  0.2544],\n",
      "        [ 0.0695, -0.0612,  0.1387,  0.0247],\n",
      "        [ 0.1826, -0.1949, -0.0365, -0.0450],\n",
      "        [ 0.0725, -0.0020,  0.4371,  0.1556],\n",
      "        [-0.1862, -0.3020, -0.0838, -0.2157],\n",
      "        [-0.1602,  0.0239,  0.2981,  0.2718],\n",
      "        [-0.4888,  0.3100,  0.1397,  0.4743],\n",
      "        [ 0.3300, -0.4556, -0.4754, -0.2412],\n",
      "        [ 0.4391, -0.0833,  0.2140, -0.2324],\n",
      "        [ 0.4906, -0.2115,  0.3750,  0.0059],\n",
      "        [-0.2634,  0.2570, -0.2654,  0.1471],\n",
      "        [-0.1444, -0.0548, -0.4807, -0.2384],\n",
      "        [ 0.2713, -0.1215,  0.4980,  0.4008],\n",
      "        [-0.0234, -0.3337,  0.3045,  0.1552],\n",
      "        [-0.3232,  0.3248,  0.3036,  0.4434],\n",
      "        [-0.2803, -0.0823, -0.0097,  0.0730],\n",
      "        [-0.3795, -0.3548,  0.2720, -0.1172],\n",
      "        [ 0.2442,  0.0285,  0.1642,  0.1099],\n",
      "        [ 0.1818,  0.2479, -0.4631,  0.2517],\n",
      "        [-0.3516, -0.3773,  0.0304, -0.0852],\n",
      "        [ 0.2937, -0.2896, -0.4445,  0.3639],\n",
      "        [-0.0741,  0.2812,  0.1607, -0.3749],\n",
      "        [ 0.1004,  0.1201, -0.3348, -0.2372],\n",
      "        [ 0.1705,  0.0896, -0.2127, -0.1514],\n",
      "        [ 0.4579, -0.0925,  0.2819,  0.2165],\n",
      "        [-0.3232, -0.4252,  0.4799,  0.0261],\n",
      "        [ 0.3427,  0.1036,  0.1608,  0.3735],\n",
      "        [ 0.4741, -0.3318,  0.0625,  0.3731],\n",
      "        [ 0.3622,  0.3106, -0.3619, -0.3601],\n",
      "        [-0.3024,  0.0628,  0.4983, -0.3158],\n",
      "        [ 0.2664, -0.2767, -0.4701, -0.1063],\n",
      "        [ 0.2881,  0.4642, -0.3105,  0.1085],\n",
      "        [ 0.4314,  0.3313,  0.3116,  0.3553],\n",
      "        [ 0.3163,  0.1291, -0.3419, -0.4199],\n",
      "        [-0.2291, -0.0582, -0.3065,  0.1829],\n",
      "        [ 0.1547, -0.1132,  0.1922,  0.1616],\n",
      "        [ 0.3053,  0.3367, -0.1693,  0.4885],\n",
      "        [-0.0578, -0.0172, -0.4719, -0.3218],\n",
      "        [-0.2921, -0.2139,  0.3555, -0.1634],\n",
      "        [-0.3736,  0.1924,  0.1601,  0.3238],\n",
      "        [-0.2587,  0.1084, -0.1820, -0.1123],\n",
      "        [-0.3985, -0.2279, -0.1531,  0.2138],\n",
      "        [ 0.0913,  0.1235,  0.4991,  0.4873],\n",
      "        [ 0.3410,  0.0159, -0.3459,  0.3908],\n",
      "        [-0.1250, -0.0404, -0.4307, -0.0988],\n",
      "        [-0.3225,  0.4595, -0.4323, -0.3897],\n",
      "        [-0.0170, -0.2704,  0.1789, -0.1925],\n",
      "        [-0.2348,  0.0283,  0.3619, -0.3517],\n",
      "        [ 0.2348,  0.3212,  0.4891, -0.3500]], requires_grad=True))\n",
      "('input_layer.bias', Parameter containing:\n",
      "tensor([ 0.1211, -0.3697,  0.4269, -0.1940,  0.3012,  0.0149, -0.0389, -0.0160,\n",
      "         0.0850,  0.2357,  0.0802,  0.1525, -0.4498,  0.3643,  0.4359,  0.4133,\n",
      "         0.3696, -0.3608, -0.1854,  0.4409, -0.3808,  0.4536, -0.3932, -0.3522,\n",
      "         0.2444, -0.3592, -0.1146,  0.3637,  0.3960,  0.4729, -0.1015, -0.3886,\n",
      "         0.4923, -0.1065, -0.2057,  0.1219, -0.3497,  0.3286,  0.3134, -0.3967,\n",
      "        -0.4107, -0.0438,  0.2100, -0.0145, -0.2535,  0.0114, -0.4700, -0.3534,\n",
      "        -0.3328,  0.4118], requires_grad=True))\n",
      "('hidden_layer.weight', Parameter containing:\n",
      "tensor([[ 0.1247, -0.0480,  0.0063,  ...,  0.0019, -0.0808, -0.0789],\n",
      "        [-0.0211, -0.0407,  0.0347,  ..., -0.0707, -0.1293, -0.0830],\n",
      "        [ 0.0864,  0.0310, -0.0499,  ...,  0.0345,  0.1268,  0.0816],\n",
      "        ...,\n",
      "        [-0.0207,  0.0017, -0.0842,  ...,  0.0175,  0.0594,  0.0292],\n",
      "        [-0.0814,  0.0773,  0.1298,  ..., -0.0934, -0.1362,  0.0787],\n",
      "        [-0.1214, -0.0171,  0.1214,  ...,  0.1301,  0.0451,  0.0690]],\n",
      "       requires_grad=True))\n",
      "('hidden_layer.bias', Parameter containing:\n",
      "tensor([-0.0605, -0.0485,  0.1029,  0.0518,  0.0355, -0.1066, -0.1129,  0.0286,\n",
      "        -0.1061,  0.0992,  0.1178, -0.0586, -0.0912,  0.1369, -0.0755, -0.1127,\n",
      "        -0.1026,  0.0246,  0.0275,  0.0913,  0.1139,  0.0603,  0.0202,  0.1277,\n",
      "        -0.0584, -0.0248,  0.1198, -0.0476,  0.0758,  0.0303],\n",
      "       requires_grad=True))\n",
      "('output_layer.weight', Parameter containing:\n",
      "tensor([[ 0.0644, -0.0417, -0.1555,  0.1309,  0.1503, -0.1450, -0.0854,  0.0077,\n",
      "          0.0905, -0.0258, -0.0800,  0.1421, -0.0917,  0.1659,  0.0031,  0.0958,\n",
      "          0.0429,  0.0321,  0.1343, -0.0092,  0.1288,  0.0603, -0.1019,  0.1197,\n",
      "         -0.1297,  0.0951, -0.0391,  0.0567, -0.0218,  0.0361]],\n",
      "       requires_grad=True))\n",
      "('output_layer.bias', Parameter containing:\n",
      "tensor([-0.0551], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터 즉, W와 b확인\n",
    "for n in ml.named_parameters(): print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n",
      "tensor([[0.3364],\n",
      "        [0.3836]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델인스턴스명 (데이터)\n",
    "# 임의의 데이터\n",
    "dataTS=torch.FloatTensor([[1,3,5,7], [2,4,6,8]])\n",
    "targetTS=torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pre_y = ml(dataTS)\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
